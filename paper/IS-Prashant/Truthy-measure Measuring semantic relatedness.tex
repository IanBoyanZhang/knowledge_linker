\documentclass[12pt]{article}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{listings}
\lstset{breaklines=true}
\usepackage{amsmath}
%\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage[margin=1in,paperwidth=8.5in,paperheight=11in]{geometry}
\usepackage{graphicx}
\begin{document}

\title{Truthy-measure: Measuring semantic relatedness using Wikipedia}
\author{Y790: Independent Study Project \\ Prashant Shiralkar}
\date{\today}
\maketitle

\section{Abstract}
Traditional ways of ascertaining the truthfulness of statements on online social media platforms like Twitter have become very challenging due to the lack of our ability to keep up with the  unprecedented rates at which people voice their opinions and post their content. To address this problem, \textit{Truthy-measure} project at the Center of Complex Networks and Systems Research is a novel attempt to automatically infer the veracity of a claim or statement being made on such platforms. In order to do that, we use a large scale semantic network derived from the knowledge encyclopedia Wikipedia, as a knowledge base for verification of statements. This independent study project studies the problem of measuring semantic relatedness of concepts or entities in the semantic network by following a network theoretic approach. It uses the measures and tools given by network science to evaluate their effectiveness in determining semantic similarity. In particular, we compare the metric, ultra-metric and diffusion-metric in their effectiveness to reason about concept similarity. Our preliminary results indicate that metric closure works slightly better than ultra-metric by minimizing distortion of the original semantic network. Although the diffusion distance metric has been shown to be promising for community detection and other network approximate reasoning tasks, its evaluation to be an effective measure for computing semantic relatedness is still a pending work. 

\section{Introduction}
The ability to voice opinions and publish content online has become simpler than ever before by the use of online social media and the widespread embracing of mobile devices by the masses. One of the consequences of this paradigm shift is the rapid proliferation and spread of bogus and irrelevant information on the online social media. Thus, it becomes necessary to wade through the pile of misinformation to isolate factual ones in near real time. The traditional approaches of ascertaining the truthfulness of statements by manual investigation or searching through traditional knowledge stores have become a challenge, thereby the situation necessitates to devise novel approaches to keep up with the pace of information generation and spread. One such approach that we have adopted is to use existing knowledge sources to verify the authenticity of content on platforms like Twitter. The problem then becomes one of assessing the veracity of a statement from online social network by using its constituent entities and measuring automatically their semantic relatedness to effectively decide whether the statement is genuine or a misinformation. 

\paragraph{•}
Wikipedia is a large scale, collaborative knowledge store that could be used to derive a semantic network needed to assess a statement. Wikipedia in particular is appealing for this purpose as it has high coverage with large amount of information and the text on Wikipedia is highly structured to be useful for computational purposes. Once we have derived the semantic network from Wikipedia, then the question is how do we measure the strength of semantic relatedness between two given entities which are part of a statement. Recent advances in network science literature have given us different measures of similarity and ways of evaluating their effectiveness for the application. These measures are well-founded with desirable axiomatic characteristics and thus could be used for network approximate reasoning on complex networks such as our semantic network based on Wikipedia. This project deals with the implementation of various metrics to determine the similarity between entities of the Wikipedia semantic network. The ultimate objective is to choose the best metric among the alternatives that effectively captures the notion of semantic relatedness to be used for automatic fact-checking of statements.

\paragraph{•}
Section 3 provides an overview of the Wikipedia and DBpedia, a project to extract structured information from Wikipedia and to make it accessible on the Web. It also describes how the proximity network representing the semantic network is constructed. Section 4 describes the measures used to evaluate similarity. Section 5 details the experiments performed as part of implementing the measures and the techniques used to address the problem of computational scalability. It also describes a data-driven approach of classification of a politician as a democrat or republican based on a set of ideologies available in the semantic network. The result is not very interesting in that none of the models perform significantly better than a random classifier nor does one measure significantly outperform another, but it does reflect, to some extent, the effectiveness of the metric used to generate the data set. Section 6 discusses the results obtained so far and section 7 summarizes the work and proposes future directions.


\section{Wikipedia and DBpedia}
\paragraph{•}
Wikipedia is a large scale, multi-lingual collaborative encyclopedia of human knowledge. The text in Wikipedia is highly structured, hence making it suitable for serving as a knowledge base in building intelligent applications for the Web. 

\paragraph{•}
DBpedia project is a community effort that extracts the structured information on Wikipedia and makes the information about the extracted entities widely available in the form of Resource Description Format (RDF) triples. The resulting entities comprise of roughly 3.4 million entities including persons, places, films, musical works and companies and 23 million connections. The RDF triples are then leveraged to conceptualize and build the semantic network. 

\paragraph{•}
The semantic network derived from Wikipedia consists of the entities representing the nodes and the actual connections between them as the edges. The RDF triples are used to produce an adjacency matrix of the entities. This matrix represents the presence or absence of connections occurring between the entities of Wikipedia. The question that needs to be addressed next is how to assign weights to the connections in the network for assessing semantic similarity. 

\paragraph{•}
The graph that is obtained from the adjacency matrix derived from Wikipedia could be called a \textit{proximity graph}. A proximity graph is a fuzzy graph which is reflexive and symmetric. This is desirable because direct distance or similarity between two entities is by definition symmetric. Moreover, due to the undirected nature of the proximity graph, it lets us leverage the isomorphism that exists between a proximity graph and a distance graph to compute transitive or indirect associations. In particular, it allows us to use the following map to construct the proximity graph based on the distance graph and vice versa:$$p_{ij} = \phi(d_{ij}) = \frac{1}{d_{ij}+1}$$ where $p_{ij} \in [0,1] $ and $\phi:[0,\inf) \rightarrow [0,1]$

\paragraph{•}
In order to compute the indirect connections between entities, we need a notion of transitivity on the proximity graph. There are infinitely many ways to define transitivity based on what are called the \textit{disjunction/conjunction} pairs in a transitive relation on a graph with itself. The choice of the disjunction/conjunction pairs defines how the transitive associations are computed. In the following section, without going into the details of the algebraic structures used to define a transitive relation, we overview three metrics that can be computed in finite time. Each of them has different implications on the information they preserve about the original network, and the amount of distortion they induce. The use of the metrics to compute the transitive connections ultimately gives us what is known as the similarity graph or transitive closure or its isomorphism called distance closure.

\section{Measures to evaluate semantic relatedness}
Recent studies in the network science research have explored the isomorphism between distance and proximity graphs, and the transitivity space that could be used for network approximate reasoning. The metrics arising therefrom are used to compute transitive closure on proximity or fuzzy graphs or the isomorphism on the distance graphs called the distance closure. The distance closure is a network with the same set of nodes as the original distance graph, but with a value for each possible pair of nodes representing the strength of their indirect association. Although the studies have shown that theoretically there are infinite number of ways to compute indirect distances or distance closures, practically, the analysis of large graphs such as our semantic network from Wikipedia involves computation of shortest paths or path lengths in the transitivity criterion. There are multiple alternatives to define the path length which result in different distance closures. These closures resulting from each of the transitivity criteria induce a level of \textit{distortion} from the original graph. Informally, distortion is the sum of differences between the proximity values in the closure and that from the original proximity graph. It is important that we choose a transitivity criterion that makes sense for the application as well as induces the smallest distortion. Moreover, it is not only desirable that the transitivity criterion used to compute transitive closure gives us values that are interpretable and intuitive, but it is also important that the closure computation converges in finite time. Not all closures that are possible can converge in finite time. Only those whose algebraic structures have specific characteristics (e.g. monoid, dioid) are guaranteed to converge. More information about the various distance closures could be found in the paper 'Distance Closures on Complex Networks' by Tiago Simas and Luis Rocha. 

\paragraph{•}
From the descriptive statistics such as a power-law degree distribution of the semantic network derived from Wikipedia, we know that Wikipedia is a scale-free network. Hence, it is plausible that there are few nodes with large number of connections, and many others with fewer connections. Degree of an entity or node is one of the network features that we have put to use to define path length for our distance closure. Specifically, the length of a path between a pair of nodes is the sum of the degree of all intermediate nodes falling on that path. The idea is to penalize heavily the intermediate nodes with large number of connections more than the ones with few connections. In other words, the more the number of entities a given entity is connected to, the more general concept the later entity may represent and hence less is its ability to discriminate and verify the specificity of information in a statement coming from a social media platform. Hence, in measuring the similarity between entity A and entity B, we want to penalize entity C falling on its way inversely proportional to its degree. This way, if there are more nodes with a large number of neighbors along the way, the further apart the two entities of interest are and hence less is their similarity. This idea is used to construct a \textit{similarity graph} from the proximity graph derived earlier. 

\paragraph{•}
In the current project, we experimented with three different metrics corresponding to different transitivity criteria, which are briefly described next. We also mention the implications and shortcomings of each of them.

\subsection{Metric}
The distance closure computed using this metric is called a metric closure. The metric closure is a distance graph $D^{mc}(X,X)$, (X is the node set) whose edges $d_{ij}^{mc}$ between two nodes $i$ and $j$ are defined by the shortest direct or indirect distance between them in the original distance graph. This metric is also called $<min, +>$ since in computing the distance between two nodes, we sum the lengths of all the paths that lie between them and then choose the path with minimum length. The implicit assumption in this approach is that the two entities are as strongly connected as the length of the shortest path between them. Also, since there is already a built-in penalty for long paths, the approach matches our intuition that entities closer in the network ought to be more related than entities farther away. Moreover, there are fewer edges from the original graph that are altered, except the indirect connections that have a smaller distance than the direct connection in the original graph. Hence, this closure imposes a weaker distortion to the original graph. The undesirable property of this metric is that its algebraic structure is too poor to define a reasonable logic. As an example, De Morgan's laws are not defined on its algebraic structure.

\subsection{Ultra-metric} 
The distance closure computed using this metric is called the ultra-metric closure. The ultra-metric closure, also called the $<max, min>$ closure of a distance graph is a graph in which the distance between two nodes is the strongest of the weakest link of all the paths that lie between them. The closure is still a shortest path closure but the definition of the path length is taken to be the weakest link of the path instead of the sum of the path segments as in the metric closure. The idea behind this closure is that two nodes are only as strongly similar as the strongest of the weakest association that exists between them. Since using this approach could result in a lot of node pairs having the same ultra-metric distance, the downside of using this metric for closure computation is that it results in high distortion to the original graph. One upside of using this metric closure from among the space of generalized metric closures is that its algebraic structures have good axiomatic characteristics to allow computation of closure in finite time.

\subsection{Diffusion metric}
The closure computed over a distance graph using this metric is called the diffusion closure. This metric combines the idea of penalizing long paths with the notion of taking into account multiple paths between two nodes through which the influence might "diffuse". The diffusion closure is a  graph in which that the distance between any pair of nodes is the harmonic mean of the length of the paths that exists between them divided by the number of paths. In general, the closure computation does not converge in finite time. Since each closure computation is a composition operation on itself using the algebraic structures corresponding to the measure used, the diffusion distance quickly converges to zero within a few iterations. What is interesting in the computation process is to see how the values converge to zero within a few iterations. If we limit the computation to a few iterations or upto a certain precision, we have a relative measure of diffusion distances for the nodes in the diffusion closure.



\section{Experiments}
This section describes the variety of approaches carried out to implement the metrics described in the previous section to compute the transitive closure (or distance closure) on the proximity graph derived from Wikipedia. This has been a work in progress to determine the best metric to use for the fact-checking task.

\subsection{Metric implementation}
Computing the metric closure entails finding all the shortest paths between every pair of nodes in the distance graph. This is known as All Pairs Shortest Paths (APSP) problem. We employed the most common approach of solving this problem using Single-Source Shortest Path (SSSP) Dijkstra's algorithm by calling it $n$ times, where n is the number of nodes in the distance graph. 

\subsection{Ultra-metric} 
The ultra-metric closure computation involves finding the \textit{bottleneck} capacity between every pair of nodes in the proximity graph. In our context, a bottleneck capacity representing the strength or proximity of an indirect connection between two nodes is the strongest among the weakest links (inverse degree values) found from different paths that exist between the two nodes. 

\paragraph{•}
Many of the approaches that were considered to implement this metric suffered from the problem of scalability, even for relatively small graphs like Zachary karate club network. There were multiple approaches tried to address the problem of scalability including the use of Python profilers, Cython which gives us nearly C speeds, alternative breadth-first strategies, etc. The first implementation to compute bottleneck shortest paths on the undirected graph was based on the algorithm described in 'On the Bottleneck Shortest Path' by Volker Kaibel and Matthias Peinhardt, which is briefly summarized in the algorithm below.

\begin{algorithm}
\begin{algorithmic}
\STATE Input: A matrix $G$ such that an edge weight $G[i,j]$ gives the penalty value incurred if $j$ is an intermediate node. Note that a direct connection has zero penalty. The dimensions of the matrix are $NxN$, and there are $m$ edges in the graph. $s$ is a source node, and $t$ is a target node. We want to compute the bottleneck shortest path distance between $s$ and $t$.
\STATE Initialize $iteration = 0$ 
\WHILE{ iteration  $<\lceil log(m) \rceil$ }
	\STATE Determine the median $M$ of the edge weights currently in the graph. 
	\STATE Delete all the edges with weight $< M$.
	\STATE If the graph is not s-t connected, find the connected components and collapse each of them into individual nodes and create a new graph with nodes representing the components and edges represented by the minimum of the edges from the original graph whose ends lay in different components.
	\STATE Else if graph is s-t connected implying the deleted edges did not contain the bottleneck edge, do nothing.
	\STATE $iteration = iteration + 1$
\ENDWHILE
\STATE Output: the last remaining edge as the bottleneck edge
\end{algorithmic}
\end{algorithm}
Since this approach computes the bottleneck distance between each pair of nodes, and finding the connected components is an expensive computation, this approach did not scale very well for our problem. As an alternative, we employed a breadth-first search strategy by implementing a modification of Prim's algorithm of finding minimum spanning tree in a graph that computes the bottleneck shortest path distances. This approach was found to be quite efficient, and we used this for our purpose. 

\subsection{Diffusion metric}
Computing a distance closure using this metric involves for every pair of nodes, computing the harmonic mean of the paths that exist between the two nodes. Since there are exponential number of paths between a pair of nodes, clearly, the computation for the large network cannot be achieved in polynomial time. Hence, we looked for approximation techniques that let us compute the diffusion distance upto a certain precision. One such approximation technique that could be exploited is that shown by F. Wu and B. A. Huberman in their paper 'Finding communities in linear time: a physics approach'. The idea is to assume that each node has certain voltage and the edges with their weights indicate the resistance between them. Two nodes in the network act as the poles of a battery. By making these assumptions, the graph is viewed as an electric circuit with current flowing along the edges. The problem is that of computing the voltages at each node by solving Kirchoff's equations and finding the absolute difference between the voltages at the source and target node. The difference of voltages at the source and target nodes is representative of the diffusion distance upto a certain precision. 

\paragraph{•}
As part of a 'calibration' process of implementing and testing various metrics on the Wikipedia-derived network and thereafter selecting the most appropriate metric to use for fact-checking, we performed the following evaluation. We created a data set of politicians in the network and represented each of them by a vector of strengths to a set of ideologies from the network. A machine learning approach was then used to build a model from the data set to classify a given politician. There were three learners used for the classification: 1) k-Nearest Neighbor, 2) Random Forest, and 3) AdaBoost. The result section describes the result for Random Forest and Adaboost. The promise in this classification task is that if the metric used is to provide us with enough discriminatory information, the precision and recall should be high. Hence, this task helps in a comparative evaluation of the metrics used.

\section{Result}
Table 1 depicts the average precision, average recall and average F-score of three classifiers on the data set consisting of 4237 democratic and republican politicians as a vector of 819 ideologies. There are 2366 democrats and 1871 republicans. For all of the three methods, a 10-fold cross-validation approach is used to train the model. The idea is to make a comparative evaluation of the metric and ultra-metric measures. Diffusion closure computation is not complete, hence only the results for metric and ultra-metric are shown in the table. The machine learning methods used in the classification include k-Nearest Neighbor, Random Forest and AdaBoost, all of which fall in the non-linear methods of classification. For Random Forest and AdaBoost, an ensemble of 200 decision trees and 0-1 loss function was used to get the majority votes for each test case.  Please note that Random Forest and boosting have not been performed on the metric closure, so only the result from k-NN is shown.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Measure & Classifier & Avg. Precision & Avg. Recall & Avg. F-score \\ \hline \hline
Ultra-metric & k-Nearest Neighbor & 50 & 34 & 40 \\ \hline
Ultra-metric & Random Forest & 57 & 85 & 68 \\ \hline
Ultra-metric & AdaBoost & 57 & 99 & 72 \\ \hline
Metric & k-Nearest Neighbor & 56 & 43 & 49 \\ \hline
Metric & Random Forest & - & - & - \\ \hline
Metric & AdaBoost & - & - & - \\ \hline

\end{tabular}
\caption{Classification results on the politician-ideology data set}
\end{table}

\paragraph{•}
Surprisingly, the Random Forest and AdaBoost models have a much better average recall than k-NN. The average precision value of metric is slightly better than ultra-metric, but whether or not it is significant still needs to be confirmed using a statistical hypothesis test. Given Table 1, it is hard to compare the relative effectiveness of both measures. Yet, the ultra-metric does not perform very well in comparison to the metric. The reason is clear that the ultra-metric does not take into account the length of the paths, which are taken into account by the metric in its definition. Thus, ultra-metric deems a lot of actual far-away concepts to be equally similar to ones only a few steps away. At the least, we could infer two things from the table. Firstly, this confirms that ultra-metric indeed results in higher distortion to the original graph than that done by metric. Secondly, it is indeed desirable that the decisive measure give us higher precision than its alternatives. In case of diffusion-closure, although this approach has been implemented using the physics-based approach as shown by F. Wu and B.A. Huberman, it has not shown useful results on test graphs such as the popular Zachary karate club network. This is a work in progress and needs to be explored if this metric is believed to be promising for measuring semantic relatedness.

\section{Conclusions and future work}
In this project, we implemented the metric, ultra-metric and diffusion-metric closures on the semantic network derived from Wikipedia. We also tested the implementations of the measure on a number of test graphs as well as the Zachary karate club network. The preliminary results shown in this study indicate that metric closure is more promising than ultra-metric for the task of measuring semantic relatedness between concepts. The evaluation of diffusion metric for this purpose could be taken up as the next step. An alternative to the physics based approach to diffusion closure computation mentioned earlier could be the computation of n-th power of the graph using matrix operations. This way, the number of paths in the diffusion distance could be controlled as well as desired level of precision could be achieved in finite time. Moreover, so far, the focus of the analysis has been only on entities that involve a single predicate such as 'A is-a B'. Considering more complex predicates in this analysis would certainly enrich the technique for its usefulness, but this probably also implies additional sophistication in the current methodology. This is indeed desirable, and hence a direction to pursue further.

\section{Acknowledgements}
I would like to express my sincere gratitude to Prof. Filippo Menczer, Prof. Alessandro Flammini and Giovanni Luca Ciampaglia for giving me the opportunity to work on this project, and for their continual advice and support during this term. I am also thankful to the Center of Complex Systems and Networks Research for providing me with the computational resources without which the project would not have been feasible.

\section{References}
\begin{enumerate}
\item Distance closures on Complex Networks. Tiago Simas, Bharat Dravid and Luis Rocha. Under consideration for publication in Network Science.
\item Finding communities in linear time: a physics approach. F. Wu and B. A. Huberman. The European Physics Journal B
\item On the Bottleneck Shortest Path Problem. Volker Kaibel and Matthias A. F. Peinhardt. Konrad-Zuse-Zentrum fur Informationstechnik Berlin
\item DBpedia - A Crystallization Point for the Web of Data. Christian Bizer, Jens Lehmann, Georgi Kobilarov, Soren Auer, Christian Becker, Richard Cyganaik and Sebastian Hellmann.
\end{enumerate}


\end{document}