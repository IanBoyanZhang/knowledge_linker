\documentclass[10pt, letterpaper, english]{nature}

\usepackage{times}
\usepackage{color}
\usepackage[demo]{graphicx}
\usepackage{booktabs}

\usepackage{hyperref}

% TODO Quote rank order correlation (Kendall/Spearman) in text or as a table
% (panel 3 in Fig4 )

\author{
    Giovanni Luca Ciampaglia\textsuperscript{1},
    Prashant Shiralkar\textsuperscript{1},
    Johan Bollen\textsuperscript{1},
    Luis M. Rocha\textsuperscript{1},
    Alessandro Flammini\textsuperscript{1},
    Filippo Menczer\textsuperscript{1}
}

\title{Can Machines Determine Truth?}
\date{}
 
\begin{document}

\maketitle

\begin{affiliations}
\item Center for Complex Networks and Systems Research, School of Computing,
	Indiana University, Bloomington, IN USA
\end{affiliations}

\begin{abstract}
	This is the abstract.
\end{abstract}

\section*{Paper outline}

\begin{enumerate}

    \item Fact-checking -- determining the truthfulness of non-fictional
        statements -- is a matter of increasing importance in today's
        information-rich world. Politics, public opinion, public health, crises,
        natural as well as man-made disasters, are all examples of domains in
        which the spreading of inaccurate or malicious information can cost
        lives and other invaluable assets, to cite a few.

    \item Traditional, journalistic fact-checking is a slow, painstaking
        activity, and it is not able to dispel misinformation in society at
        large, as sometimes notable examples from politics
        \cite{VariousAuthors2010,Ratkiewicz2011}, urban legends
        \cite{Snopes.com}, and online rumors \cite{Friggeri2014} show.

    \item Therefore, even small improvements toward automating the practice of
        fact-checking are in great need \textcolor{red}{[VAGUE; RHETORICAL]}.

    \item Nowadays, massive online repositories provide a wealth of information
        about the sum of all human knowledge. Here we show that the knowledge
        stored into these global brains can be leveraged by computational tools
        for the purpose of fact-checking. 
        
    \item As a proof of concept, we present a method that uses structured
        information extracted from Wikipedia. Using this data we build a
        large-scale ($|V| = 3.14{\rm M}, |E| = 23{\rm M}$) knowledge network in
        which nodes are entities and edges are facts connecting them (see
        Figure~\ref{fig:fig1_diagram}).

    \item We ask whether the truth value of a novel, untested statement about
        any two entities in our network can be correctly inferred from the
        knowledge present in it. To do so we define a measure of semantic
        proximity (i.e., with values in $\left[ 0, 1 \right]$) based on shortest
        paths and on the intuitive heuristic that, because of their high
        generality, high-degree entities (e.g. `United States', `Male',
        `Female') should provide little evidence in support of any given
        statement that makes use of them. Conversely, entities with a low degree
        are likely to be more specific, and thus should be weighted more, see
        Figure~\ref{fig:fig1_diagram}. 
        
    \item Instead of simply retrieving the information already available in the
        network, our method thus synthesizes new facts from it. It does so in a
        way inspired to the principle of epistemic closure, by finding evidence
        of indirect connections via paths on the graph. The proximity measure is
        isomorphic to a distance-based graph closure under suitable metrics
        \textcolor{red}{(see Supplementary Information)}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/fig1_diagram.pdf}
    \caption{\textbf{Graph closure for fact-checking}. Screenshot of infobox
    from `Barack Obama' page and diagram with node and edges. Explanation of the
    metric: schematic diagram of paths and high-degree vs low-degree nodes.}
    \label{fig:fig1_diagram}
\end{figure}

    \item This simple proximity measure can be interpreted as a generic strength
        of association between two concepts, so it is legitimate to ask what
        kind of information it provides and whether it can be used for
        fact-checking at all. As a preliminary step, we ascertain that the truth
        scores produced by our method contain a useful signal. We do so with a
        binary classification task.

    \item We consider all members of the 112\textsuperscript{th} US Congress
        affiliated to either the Democratic or Republican party (Senate: $N =
        100$; House: $N = 445$) and compute the proximity score of each
        congressperson to any entity representing an ideology ($M=819$). Given a
        politician $X$ and an ideology $Y$ the score can be interpreted as the
        truth value of the statement ``$X$ endorses ideology $Y$''. Each
        politician is thus represented by a vector of coordinates in a
        high-dimension ideological space (see
        Figure~\ref{fig:fig2_classification}, top). 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/fig2_classification_panel1.pdf}
    \includegraphics[width=\textwidth]{images/fig2_classification_panel2.pdf}
    \caption{\textbf{Top}. Local structure of politician-ideology
    network (with paths) \textbf{Bottom}. DWNOMINATE comparison plot. Table
    with F1, AUC for random forests and nearest neighbors on all combos of
    metric/weight/\{directed,undirected\}} 
    \label{fig:fig2_classification}
\end{figure}

        
    \item We feed this $N\times M$ feature matrix into standard binary
        classification models ($k$-Nearest Neighbors and Random Forests) and see
        if it contains a signal capable to recover the party affiliation of each
        member of Congress. Results show that the performances of models trained
        on our truth scores compare to the state of the art in political
        classification \cite{Poole2007}, which is based instead on a
        comprehensive database of roll call votes, see
        Figure~\ref{fig:fig2_classification}, bottom. If instead of using the
        graph closure we use only the information provided by the direct
        neighbors of an entity, than the models do only slightly better than a
        random classifier, showing that most of the signal comes from the
        indirect information that the graph closure is able to recover.

    \item Having ascertained that our method produces a useful signal, we
        evaluate whether it can be used for general-purpose fact-checking. One
        requirement for a fact-checker is that it must be able to do without
        \emph{a priori} information. In the previous task we trained the model
        with input data coming from a pre-defined set of features (ideologies)
        that were known to be relevant for the domain of the problem (politics).
        In the following, we evaluate the performance of our method in two
        controlled scenarios characterized by decreasing levels of \emph{a
        priori} information.
     
    \item An additional requirement is that the facts to be checked must not be
        trivially true, i.e., they should not already be present in the database
        from which we build the network. In the following, whenever we compute
        the truth score of a fact about two entities, we first check if they are
        connected by an edge and, if this is the case, we remove it from the
        network before running the fact-checker. In this way we can see if the
        fact-checker is able to detect from the remaining knowledge that the
        edge is missing.

    \item In the first benchmark we select a subject (e.g. Geography) and see if
        the fact-checker is able to respond correctly to questions about it
        (e.g. ``is Paris the capital of France?''). We consider four topics and
        for each topic $N$ multiple-choice questions, each with $N$ possible
        answers: US presidential couples ($N = 17$), world capitals ($N = 187$),
        US state capitals ($N = 48$), and Academy Awards for Best Director ($N =
        59$). For example, in the first dataset, we ask the fact-checker to
        correctly indicate, among all, the spouse of any of the last seventeen
        presidents of the United States. In the latter, we ask the fact-checker
        what is the movie for which a director won the Oscar. In
        Figure~\ref{fig:fig3_confusion} we show $N\times N$ confusion matrices
        obtained by computing the proximity score for each question-choice
        combination. Each question corresponds to a row of the matrix. We find
        that the fact checker selects the correct answer 82.4\%, 87.2\%, 97.9\%,
        and 40.7\% of the times, respectively. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/fig3_confusion.pdf}
    \caption{\textbf{Multiple-choice questions}. Four confusion matrices:
    oscars, capitals, presidents, countries.} 
    \label{fig:fig3_confusion}
\end{figure}

    \item In the second benchmark we dispose of the implicit information
        contained in the multiple-choice setting and confront the fact-checker
        with yes-or-no questions, e.g., was George Washington born in
        Westmoreland County, VA? As ground truth we use the Google Relation
        Extraction Corpus (GREC)\cite{Google2013}. This data is also extracted
        from Wikipedia, but whereas the facts in our database come only from the
        infobox of the page, the GREC is based upon information from the whole
        text of an entry (see Figure~\ref{fig:fig4_relation}, top). 
        
    \item In the GREC, each relation was assessed independently by five human
        raters (Figure~\ref{fig:fig4_relation}, top), which could read the
        Wikipedia page and follow any of its link before making their decision.
        Therefore, while our fact-checker has access to the set of factual links
        provided by the infoboxes, the knowledge network that the raters had
        access to for seeking evidence of indirect connectivity was much larger
        \textcolor{red}{($|E| = XX\mathrm{M}$ as of 2014-06)}.
        
    \begin{figure} \centering
        \includegraphics[width=\textwidth]{images/fig4_relation_panel1.pdf}
        \includegraphics[width=\textwidth]{images/fig4_relation_panel2.pdf}
        \caption{\textbf{Relation extraction task}: Relation extraction task.
        \textbf{Top}: diagram with snapshot from Wikipedia page text with
        sentence highlighted, mapped entities and actual ratings.
        \textbf{Bottom}. ROC curve plot (closure with edge removal, noclosure).}
        \label{fig:fig4_relation} \end{figure}

\begin{table}
    \centering
    \begin{tabular}{lllll}
        \toprule
        Relation & Spearman $\rho$ & $p$-value & Kendall $\tau$ & $p$-value \\
        \midrule
        Degree & $0.17$ & $2.40\times 10^{-5}$ & $0.13$ & $9.60\times 10^{-7}$ \\
        Institution & $0.09$ & $3.80\times 10^{-19}$ & $0.07$ & $1.20\times 10^{-24}$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Relation extraction benchmark}.}
    \label{tab:relation}
\end{table}

    \item We focused on two relations from the GREC corpus: education degree
        (`degree', $N = 602$) and the institution of affiliation (`institution',
        $N = 10,726$). We map GREC ratings into an ordinal scale between $-5$
        (all raters replied `No') and $5$ (all raters replied `Yes') and compare
        them to the truth scores computed by the fact-checker, finding a
        positive signal for both relations (Table~\ref{tab:relation}). To
        quantify this signal in terms of the performances of a binary
        classifier, we transform the ground truth ratings into `yes/no' answers
        by taking the majority vote of the ratings. We find that the
        fact-checker outperforms a random classifier, see
        Figure~\ref{fig:fig4_relation}, bottom. In comparison, the original
        graph without closure performs worse than our method and closer to
        random choice ($\mathrm{AUC} = 0.47$, `degree', $\mathrm{AUC} = 0.52$,
        `institution').
    
    \item This shows that even with a relatively smaller amount of information
        at its disposal \textcolor{red}{(approximately $XX$ times less)}, our
        fact-checker is able to correctly find new facts about the real world by
        looking at indirect paths between entities. 

    \item \textcolor{red}{Cherry-picked examples of fact-checked statements
        Twitter}.
        
\end{enumerate}

\begin{addendum}
\item[Acknowledgements] The authors would like to thank \dots
    \textcolor{red}{ADD GRANTS INFORMATION}.
 \item[Competing Interests] The authors declare that they have no
competing financial interests.
 \item[Correspondence] Correspondence and requests for materials
should be addressed to G.L.C.~(email: gciampag@indiana.edu).
\end{addendum}

% \section{Introduction}
% 
% The introduction is basically the outline expanded.
% 
% \section{Related Work}
% 
% Review of literature.
% 
% \section{Methods}
% 
% This is the methods section. It is divided into two parts.
% 
% \subsection{DBpedia knowledge network}
% 
% Here we talk about DBPedia.
% 
% \subsection{Epistemic closure}
% 
% Here we talk about epistemic closures: metrics, distance, weights, undir/dir.
% 
% \section{Results}
% 
% Here we present the results. The section consists of three sub-sections.
% 
% \subsection{Calibration}
% 
% Here we describe the results of the politicians-ideology classification task.
% 
% \subsection{Validation}
% 
% Here we describe the performance of the method in the Google Relation Extraction
% corpus.
% 
% \subsection{Examples from Social media}
% 
% Here we show some examples extracted from Twitter.
% 
% \section{Discussion}
% 
% Here we evaluate the results and conclude.
% 
\bibliographystyle{naturemag}
\bibliography{biblio}
\end{document}

% vim: set sts=4 sw=4 expandtab nowrap:
